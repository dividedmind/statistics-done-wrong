@article{Ioannidis:2005bw,
author = {Ioannidis, John P A},
title = {{Why Most Published Research Findings Are False}},
journal = {PLoS Medicine},
year = {2005},
volume = {2},
number = {8},
pages = {e124},
month = jan,
doi = {10.1371/journal.pmed.0020124},
read = {Yes},
rating = {0},
date-added = {2011-02-23T01:01:35GMT+00:00},
date-modified = {2013-04-19T23:54:00GMT+00:00}
}

@article{Tsang:2009iw,
author = {Tsang, Ruth and Colley, Lindsey and Lynd, Larry D},
title = {{Inadequate statistical power to detect clinically significant differences in adverse event rates in randomized controlled trials}},
journal = {Journal of Clinical Epidemiology},
year = {2009},
volume = {62},
number = {6},
pages = {609--616},
month = jun,
doi = {10.1016/j.jclinepi.2008.08.005},
language = {English},
read = {Yes},
rating = {0},
date-added = {2012-05-12T04:43:18GMT+00:00},
date-modified = {2013-04-20T03:53:36GMT+00:00},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0895435608002217}
}

@article{Moher:1994,
author = {Moher D and Dulberg CS and Wells GA},
title = {Statistical power, sample size, and their reporting in randomized controlled trials},
journal = {JAMA},
volume = {272},
number = {2},
pages = {122-124},
year = {1994},
doi = {10.1001/jama.1994.03520020048013},
URL = {http://dx.doi.org/10.1001/jama.1994.03520020048013},
}
@article{Bedard:2007dy,
author = {Bedard, P L and Krzyzanowska, M K and Pintilie, M and Tannock, I F},
title = {{Statistical Power of Negative Randomized Controlled Trials Presented at American Society for Clinical Oncology Annual Meetings}},
journal = {Journal of Clinical Oncology},
year = {2007},
volume = {25},
number = {23},
pages = {3482--3487},
month = jul,
doi = {10.1200/JCO.2007.11.3670},
language = {English},
read = {Yes},
rating = {0},
date-added = {2012-05-12T04:44:38GMT+00:00},
date-modified = {2013-04-20T04:51:51GMT+00:00},
abstract = {More than half of negative RCTs presented at ASCO annual meetings do not have an adequate sample to detect a medium-size treatment effect.},
}

@article{Brown:1987uu,
author = {Brown, C G and Kelen, G D and Ashton, J J and Werman, H A},
title = {{The beta error and sample size determination in clinical trials in emergency medicine}},
journal = {Annals of Emergency Medicine},
year = {1987},
volume = {16},
number = {2},
pages = {183--187},
month = feb,
pmid = {3800094},
language = {English},
read = {Yes},
rating = {0},
date-added = {2012-05-23T16:32:33GMT+00:00},
date-modified = {2013-04-20T04:52:38GMT+00:00},
abstract = {In the analysis of a clinical trial an investigator may fail to discern a statistically significant difference in outcome between control and experimental groups, when in fact one exists. Failure to demonstrate such a difference when it actually exists is known as "type II" error, and its probability of occurring is termed "beta." The purpose of our study was to determine the distribution of beta errors in negative trials in the Journal of the American College of Emergency Physicians (JACEP) (1972-1979) and Annals of Emergency Medicine (1980-1984). All negative comparative clinical trials appearing in JACEP and Annals from volume 1 (1972) to volume 13 (1984) were surveyed and were eligible for inclusion in the study. A trial was defined as negative if the investigator specifically stated that there was no significant difference in outcome between the experimental and control groups. For each negative trial the following parameters were calculated: beta error, based on the sample size used and the difference determined to be important to detect clinically; sample size required to detect a clinically meaningful difference as determined by the authors of this study; and minimum true difference that had to be detected in the trial at a beta equal to 0.20, to discern a statistically significant result. For the 13 years surveyed, we found 21 endpoints in 14 negative trials that were analyzable. Only one of the trials (7.1\%) addressed the issues of beta errors and sample size determination. In the remaining 13 negative trials, the calculated beta error ranged from .60 to .97. For the endpoints analyzed, a sample size of up to 450 times larger than that used would have been required to detect a clinically important difference.},
url = {http://eutils.ncbi.nlm.nih.gov/entrez/eutils/elink.fcgi?dbfrom=pubmed&id=3800094&retmode=ref&cmd=prlinks}
}

@article{Chung:1998ku,
author = {Chung, K C and Kalliainen, L K and Hayward, R A},
title = {{Type II (beta) errors in the hand literature: the importance of power}},
journal = {The Journal of Hand Surgery},
year = {1998},
volume = {23},
number = {1},
pages = {20--25},
month = jan,
affiliation = {Section of Plastic and Reconstructive Surgery, University of Michigan Medical Center, Ann Arbor 48109-0340, USA.},
doi = {10.1016/S0363-5023(98)80083-X},
pmid = {9523949},
language = {English},
read = {Yes},
rating = {0},
date-added = {2012-05-23T16:31:05GMT+00:00},
date-modified = {2013-04-20T04:53:28GMT+00:00},
abstract = {When a study concludes that there is no difference between 2 treatments ("negative studies"), it is essential to determine whether the study has sufficient power to find a clinically significant difference. Insufficient power precludes an adequate assessment of therapeutic efficacy and may result in a type II error, an erroneous conclusion that the null hypothesis is correct. In evaluating 39 studies that highlighted negative findings in The Journal of Hand Surgery, we found that 32 (82\%) papers had a power of less than .80 to detect a 25\% treatment effect and, when the treatment effect was increased to 50\%, more than one half of the studies still had a power of 0.80. These "negative studies" frequently have inadequate statistical power to support their conclusions. These findings have important implications for researchers, editors, and readers.}
}

@article{Goodman:1999tx,
author = {Goodman, S N},
title = {{Toward evidence-based medical statistics. 1: The P value fallacy}},
journal = {Annals of Internal Medicine},
year = {1999},
volume = {130},
number = {12},
pages = {995--1004},
month = jun,
affiliation = {Johns Hopkins University School of Medicine, Baltimore, Maryland, USA. sgoodman@jhu.edu},
pmid = {10383371},
language = {eng},
read = {Yes},
rating = {0},
date-added = {2011-02-23T01:01:27GMT+00:00},
date-modified = {2013-04-20T05:23:56GMT+00:00},
abstract = {An important problem exists in the interpretation of modern medical research data: Biological understanding and previous research play little formal role in the interpretation of quantitative results. This phenomenon is manifest in the discussion sections of research articles and ultimately can affect the reliability of conclusions. The standard statistical approach has created this situation by promoting the illusion that conclusions can be produced with certain "error rates," without consideration of information from outside the experiment. This statistical approach, the key components of which are P values and hypothesis tests, is widely perceived as a mathematically coherent approach to inference. There is little appreciation in the medical community that the methodology is an amalgam of incompatible elements, whose utility for scientific inference has been the subject of intense debate among statisticians for almost 70 years. This article introduces some of the key elements of that debate and traces the appeal and adverse impact of this methodology to the P value fallacy, the mistaken idea that a single number can capture both the long-run outcomes of an experiment and the evidential meaning of a single result. This argument is made as a prelude to the suggestion that another measure of evidence should be used--the Bayes factor, which properly separates issues of long-run behavior from evidential strength and allows the integration of background knowledge with statistical findings.}
}

@article{Lazic:2010fc,
author = {Lazic, Stanley E},
title = {{The problem of pseudoreplication in neuroscientific studies: is it affecting your analysis?}},
journal = {BMC Neuroscience},
year = {2010},
volume = {11},
pages = {5},
affiliation = {Department of Applied Mathematics and Theoretical Physics, Cambridge Computational Biology Institute, University of Cambridge, Cambridge CB3 0WA, UK. stan.lazic@cantab.net},
doi = {10.1186/1471-2202-11-5},
pmid = {20074371},
language = {English},
read = {Yes},
rating = {0},
date-added = {2012-07-29T00:50:20GMT+00:00},
date-modified = {2013-04-20T05:27:05GMT+00:00},
url = {http://eutils.ncbi.nlm.nih.gov/entrez/eutils/elink.fcgi?dbfrom=pubmed&id=20074371&retmode=ref&cmd=prlinks}
}

@article{Kramer:2005in,
author = {Kr{\"a}mer, Walter and Gigerenzer, Gerd},
title = {{How to Confuse with Statistics or: The Use and Misuse of Conditional Probabilities}},
journal = {Statistical Science},
year = {2005},
volume = {20},
number = {3},
pages = {223--230},
month = aug,
doi = {10.1214/088342305000000296},
language = {English},
read = {Yes},
rating = {0},
date-added = {2012-05-23T21:42:16GMT+00:00},
date-modified = {2013-04-20T05:30:05GMT+00:00},
abstract = {This article shows by various examples how consumers of statistical information may be confused when this information is presented in terms of conditional probabilities. It also shows how this confusion helps others to lie with statistics, and it suggests both confusion and lies can be exposed by using alternative modes of conveying statistical information.},
url = {http://projecteuclid.org/Dienst/getRecord?id=euclid.ss/1124891288/}
}

@article{Bramwell:2006er,
author = {Bramwell, R and West, H},
title = {{Health professionals' and service users' interpretation of screening test results: experimental study}},
journal = {BMJ},
year = {2006},
month = jul,
doi = {10.1136/bmj.38884.663102.AE},
read = {Yes},
rating = {0},
date-added = {2012-05-23T21:38:21GMT+00:00},
date-modified = {2013-04-20T05:32:38GMT+00:00},
abstract = {Most stakeholders in pregnancy screening draw incorrect inferences from probabilistic information, and health professionals need to be aware of the difficulties that both they and their patients have with such information. Moreover, they should be aware that different people make different mistakes and that ways of conveying information that help some people will not help others.},
url = {http://www.bmj.com/content/333/7562/284.short}
}

@article{Smith:1987uz,
author = {Smith, D G and Clemens, J and Crede, W and Harvey, M and Gracely, E J},
title = {{Impact of multiple comparisons in randomized clinical trials}},
journal = {The American Journal of Medicine},
year = {1987},
volume = {83},
number = {3},
pages = {545--550},
month = sep,
affiliation = {Department of Medicine, Temple University School of Medicine, Philadelphia, Pennsylvania 19140.},
pmid = {3661589},
rating = {0},
date-added = {2011-02-23T01:01:44GMT+00:00},
date-modified = {2013-04-20T05:33:30GMT+00:00},
abstract = {The randomized clinical trial is the preferred research design for evaluating competing diagnostic and therapeutic alternatives, but confidence in the conclusions from a randomized clinical trial depends on the authors' attention to acknowledged methodologic and statistical standards. This survey assessed the level of attention to the problem of multiple comparisons in the analyses of contemporary randomized clinical trials. Of the 67 trials surveyed, 66 (99 percent) performed multiple comparisons with a mean of 30 therapeutic comparisons per trial. When criteria for statistical impairment were applied, 50 trials (75 percent) had the statistical significance of at least one comparison impaired by the problem of multiple comparisons, and 15 (22 percent) had the statistical significance of all comparisons impaired by the problem of multiple comparisons. Although some statistical techniques are available, there still exists a great need for future work to clarify further the problem of multiple comparisons and determine how the impact of this problem can best be minimized in subsequent research.}
}

@article{Bennett:2010uh,
author = {Bennett, CM and Baird, AA and Miller, MB and Wolford, GL},
title = {{Neural Correlates of Interspecies Perspective Taking in the Post-Mortem Atlantic Salmon: An Argument For Proper Multiple Comparisons Correction}},
journal = {Journal of Serendipitous and Unexpected Results},
year = {2010},
volume = {1},
number = {1},
pages = {1--5},
month = oct,
read = {Yes},
rating = {0},
date-added = {2012-05-24T23:28:04GMT+00:00},
date-modified = {2013-04-20T05:34:51GMT+00:00},
abstract = {With the extreme dimensionality of functional neuroimaging data comes extreme risk for false positives. Across the 130, 000 voxels in a typical fMRI volume the probability of at least one false positive is almost certain. Proper correction for multiple comparisons should be completed during the analysis of these datasets, but is often ignored by investigators. To highlight the danger of this practice we completed an fMRI scanning session with a post-mortem Atlantic Salmon as the subject. The salmon was shown the same social perspectivetaking task that was later administered to a group of human subjects. Statistics that were uncorrected for multiple comparisons showed active voxel clusters in the salmon's brain cavity and spinal column. Statistics controlling for the familywise error rate (FWER) and false discovery rate (FDR) both indicated that no active voxels were present, even at relaxed statistical thresholds. We argue that relying on standard statistical thresholds (p < 0.001) and low minimum cluster sizes (k > 8) is an ineffective control for multiple comparisons. We further argue that the vast majority of fMRI studies should be utilizing proper multiple comparisons correction as standard practice when thresholding their data.}
}

@article{Carp:2012ba,
author = {Carp, Joshua},
title = {{The secret lives of experiments: methods reporting in the fMRI literature}},
journal = {Neuroimage},
year = {2012},
volume = {63},
number = {1},
pages = {289--300},
affiliation = {University of Michigan, Department of Psychology, 530 Church Street, Ann Arbor, MI, 48109, USA. jmcarp@umich.edu},
doi = {10.1016/j.neuroimage.2012.07.004},
pmid = {22796459},
read = {Yes},
rating = {0},
date-added = {2013-04-15T17:37:06GMT+00:00},
date-modified = {2013-04-20T15:51:05GMT+00:00},
abstract = {Replication of research findings is critical to the progress of scientific understanding. Accordingly, most scientific journals require authors to report experimental procedures in sufficient detail for independent researchers to replicate their work. To what extent do research reports in the functional neuroimaging literature live up to this standard? The present study evaluated methods reporting and methodological choices across 241 recent fMRI articles. Many studies did not report critical methodological details with regard to experimental design, data acquisition, and analysis. Further, many studies were underpowered to detect any but the largest statistical effects. Finally, data collection and analysis methods were highly flexible across studies, with nearly as many unique analysis pipelines as there were studies in the sample. Because the rate of false positive results is thought to increase with the flexibility of experimental designs, the field of functional neuroimaging may be particularly vulnerable to false positives. In sum, the present study documented significant gaps in methods reporting among fMRI studies. Improved methodological descriptions in research reports would yield significant benefits for the field.},
url = {http://eutils.ncbi.nlm.nih.gov/entrez/eutils/elink.fcgi?dbfrom=pubmed&id=22796459&retmode=ref&cmd=prlinks}
}

@article{Gelman:2006bj,
author = {Gelman, Andrew and Stern, Hal},
title = {{The Difference Between ``Significant'' and ``Not Significant'' is not Itself Statistically Significant}},
journal = {The American Statistician},
year = {2006},
volume = {60},
number = {4},
pages = {328--331},
month = nov,
doi = {10.1198/000313006X152649},
language = {English},
read = {Yes},
rating = {0},
date-added = {2011-09-10T17:26:08GMT+00:00},
date-modified = {2013-04-20T15:57:49GMT+00:00},
abstract = {It is common to summarize statistical comparisons by declarations of statistical significance or nonsignificance. Here we discuss one problem with such declarations, namely that changes in statistical significance are often not themselves statistically significant. By this, we are not merely making the commonplace observation that any particular threshold is arbitrary---for example, only a small change is required to move an estimate from a 5.1% significance level to 4.9%, thus moving it into statistical significance. Rather, we are pointing out that even large changes in significance levels can correspond to small, nonsignificant changes in the underlying quantities.
The error we describe is conceptually different from other oft- cited problems---that statistical significance is not the same as practical importance, that dichotomization into significant and nonsignificant results encourages the dismissal of observed differences in favor of the usually less interesting null hypothesis of no difference, and that any particular threshold for declaring significance is arbitrary. We are troubled by all of these concerns and do not intend to minimize their importance. Rather, our goal is to bring attention to this additional error of interpretation. We illustrate with a theoretical example and two applied examples. The ubiquity of this statistical error leads us to suggest that students and practitioners be made more aware that the difference between ``significant'' and ``not significant'' is not itself statistically significant.},
url = {http://pubs.amstat.org/doi/abs/10.1198/000313006X152649}
}

@article{Nieuwenhuis:2011dm,
author = {Nieuwenhuis, Sander and Forstmann, Birte U and Wagenmakers, Eric-Jan},
title = {{Erroneous analyses of interactions in neuroscience: a problem of significance}},
journal = {Nature Neuroscience},
year = {2011},
volume = {14},
number = {9},
pages = {1105--1109},
month = aug,
publisher = {Nature Publishing Group},
doi = {10.1038/nn.2886},
read = {Yes},
rating = {0},
date-added = {2011-09-10T17:24:16GMT+00:00},
date-modified = {2013-04-20T15:58:45GMT+00:00},
abstract = {In theory, a comparison of two experimental effects requires a statistical test on their difference. In practice, this comparison is often based on an incorrect procedure involving two separate tests in which researchers conclude that effects differ when one effect is significant (P < 0.05) but the other
is not (P > 0.05). We reviewed 513 behavioral, systems and cognitive neuroscience articles in five top-ranking journals (Science, Nature, Nature Neuroscience, Neuron and The Journal of Neuroscience) and found that 78 used the correct procedure and 79 used the incorrect procedure. An additional analysis suggests that incorrect analyses of interactions are even more common in cellular and molecular neuroscience. We discuss scenarios in which the erroneous procedure is particularly beguiling.},
url = {http://dx.doi.org/10.1038/nn.2886}
}

@article{Bogaert:2006tc,
author = {Bogaert, Anthony F},
title = {{Biological versus nonbiological older brothers and men's sexual orientation}},
journal = {PNAS},
year = {2006},
volume = {103},
number = {28},
pages = {10771--10774},
publisher = {National Acad Sciences},
rating = {0},
date-added = {2013-04-20T16:00:12GMT+00:00},
date-modified = {2013-04-20T16:00:46GMT+00:00},
url = {http://www.pnas.org/content/103/28/10771.short}
}

@article{Simmons:2011iw,
author = {Simmons, J P and Nelson, L D and Simonsohn, U},
title = {{False-Positive Psychology: Undisclosed Flexibility in Data Collection and Analysis Allows Presenting Anything as Significant}},
journal = {Psychological Science},
year = {2011},
volume = {22},
number = {11},
pages = {1359--1366},
month = oct,
doi = {10.1177/0956797611417632},
language = {English},
read = {Yes},
rating = {0},
date-added = {2012-02-23T04:44:25GMT+00:00},
date-modified = {2013-04-20T16:03:58GMT+00:00},
abstract = {In this article, we accomplish two things. First, we show that despite empirical psychologists' nominal endorsement of a low rate of false-positive findings (≤ .05), flexibility in data collection, analysis, and reporting dramatically increases actual false-positive rates. In many cases, a researcher is more likely to falsely find evidence that an effect exists than to correctly find evidence that it does not. We present computer simulations and a pair of actual experiments that demonstrate how unacceptably easy it is to accumulate (and report) statistically significant evidence for a false hypothesis. Second, we suggest a simple, low-cost, and straightforwardly effective disclosure-based solution to this problem. The solution involves six concrete requirements for authors and four guidelines for reviewers, all of which impose a minimal burden on the publication process.},
url = {http://pss.sagepub.com/lookup/doi/10.1177/0956797611417632}
}

@article{Todd:2001hg,
author = {Todd, S. and Whitehead, A. and Stallard, N. and Whitehead, J.},
title = {{Interim analyses and sequential designs in phase III studies}},
journal = {British Journal of Clinical Pharmacology},
year = {2001},
volume = {51},
number = {5},
pages = {394--399},
publisher = {Wiley Online Library},
doi = {10.1046/j.1365-2125.2001.01382.x},
read = {Yes},
rating = {0},
date-added = {2012-06-03T14:10:02GMT+00:00},
date-modified = {2013-04-20T16:04:48GMT+00:00},
abstract = {Recruitment of patients to a clinical trial usually occurs over a period of time, resulting in the steady accumulation of data throughout the trial's duration. Yet, according to traditional statistical methods, the sample size of the trial should be determined in advance, and data collected on all subjects before analysis proceeds. For ethical and economic reasons, the technique of sequential testing has been developed to enable the examination of data at a series of interim analyses. The aim is to stop recruitment to the study as soon as there is sufficient evidence to reach a firm conclusion. In this paper we present the advantages and disadvantages of conducting interim analyses in phase III clinical trials, together with the key steps to enable the successful implementation of sequential methods in this setting. Examples are given of completed trials, which have been carried out sequentially, and references to relevant literature and software are provided.},
url = {http://onlinelibrary.wiley.com/doi/10.1046/j.1365-2125.2001.01382.x/full}
}

@article{Ioannidis:2008dy,
author = {Ioannidis, John P A},
title = {{Why Most Discovered True Associations Are Inflated}},
journal = {Epidemiology},
year = {2008},
volume = {19},
number = {5},
pages = {640--648},
month = sep,
doi = {10.1097/EDE.0b013e31818131e7},
read = {Yes},
rating = {0},
date-added = {2011-02-23T01:01:29GMT+00:00},
date-modified = {2013-04-20T16:06:08GMT+00:00},
abstract = {Newly discovered true (non-null) associations often have inflated effects compared with the true effect sizes. I discuss here the main reasons for this inflation. First, theoretical considerations prove that when true discovery is claimed based on crossing a threshold of statistical significance and the discovery study is underpowered, the observed effects are expected to be inflated. This has been demonstrated in various fields ranging from early stopped clinical trials to genome-wide associations. Second, flexible analyses coupled with selective reporting may inflate the published discovered effects. The vibration ratio (the ratio of the largest vs. smallest effect on the same association approached with different analytic choices) can be very large. Third, effects may be inflated at the stage of interpretation due to diverse conflicts of interest. Discovered effects are not always inflated, and under some circumstances may be deflated---for example, in the setting of late discovery of associations in sequentially accumulated overpowered evidence, in some types of misclassification from measurement error, and in conflicts causing reverse biases. Finally, I discuss potential approaches to this problem. These include being cautious about newly discovered effect sizes, considering some rational down-adjustment, using analytical methods that correct for the anticipated inflation, ignoring the magnitude of the effect (if not necessary), conducting large studies in the discovery phase, using strict protocols for analyses, pursuing complete and transparent reporting of all results, placing emphasis on replication, and being fair with interpretation of results.}
}

@article{Ioannidis:2005gy,
author = {Ioannidis, John P A},
title = {{Contradicted and initially stronger effects in highly cited clinical research}},
journal = {JAMA},
year = {2005},
volume = {294},
number = {2},
pages = {218--228},
doi = {10.1001/jama.294.2.218},
read = {Yes},
rating = {0},
date-added = {2011-02-23T01:01:34GMT+00:00},
date-modified = {2013-04-20T16:06:44GMT+00:00},
abstract = {Contradiction and initially stronger effects are not unusual in highly cited research of clinical interventions and their outcomes. The extent to which high citations may provoke contradictions and vice versa needs more study. Controversies are most common with highly cited nonrandomized studies, but even the most highly cited randomized trials may be challenged and refuted over time, especially small ones.}
}

@article{Ioannidis:2005bj,
author = {Ioannidis, John P A and Trikalinos, Thomas A},
title = {{Early extreme contradictory estimates may appear in published research: the Proteus phenomenon in molecular genetics research and randomized trials}},
journal = {Journal of Clinical Epidemiology},
year = {2005},
volume = {58},
number = {6},
pages = {543--549},
month = jun,
affiliation = {Institute for Clinical Research and Health Policy Studies, Department of Medicine, Tufts-New England Medical Center, Tufts University School of Medicine, Boston, MA 02111, USA. jioannid@cc.uoi.gr},
doi = {10.1016/j.jclinepi.2004.10.019},
pmid = {15878467},
language = {eng},
read = {Yes},
rating = {0},
date-added = {2011-02-23T01:49:15GMT+00:00},
date-modified = {2013-04-20T16:07:13GMT+00:00},
abstract = {BACKGROUND AND OBJECTIVE: Divergent results on the same scientific question generate controversy. We hypothesized that controversial data are attractive to investigators and editors, and thus the most extreme, opposite results would appear very early rather than late, as data accumulate, provided data can be generated rapidly.

METHODS: We used data from MEDLINE-indexed meta-analyses of case-control studies on genetic associations (retrospective, hypothesis-generating research with usually rapid turnaround) and meta-analyses of randomized trials of health care interventions (prospective, targeted research that usually takes longer) sampled from the Cochrane Library. Using cumulative meta-analysis, we evaluated how the between-study variance for studies on the same question changed over time and at what point the studies with the most extreme results ever observed had been published.

RESULTS: The maximal between-study variance was more likely to be recorded early in the 44 eligible meta-analyses of genetic associations than in the 37 meta-analyses of health care interventions (P = .013). At the time of the first heterogeneity assessment, the most favorable-ever result in support of a specific association was more likely to appear than the least favorable-ever result (22 vs. 10, P = .017); the opposite was seen at the second heterogeneity assessment (15 vs. 5, P = .031). Such a sequence of extreme opposite results was not seen in the clinical trials meta-analyses. The estimated between-study variance decreased over time in genetic association studies (P = .010), but not in clinical trials (P = .30).

CONCLUSION: In contrast to prospective trials, a rapid early sequence of extreme, opposite results is frequent in retrospective hypothesis-generating molecular research.}
}

@article{Bassler:2010ds,
author = {Bassler, Dirk and Briel, Matthias and Montori, Victor M and Lane, Melanie and Glasziou, Paul and Zhou, Qi and Heels-Ansdell, Diane and Walter, Stephen D and Guyatt, Gordon H},
title = {{Stopping Randomized Trials Early for Benefit and Estimation of Treatment Effects: Systematic Review and Meta-regression Analysis}},
journal = {JAMA},
year = {2010},
volume = {303},
number = {12},
pages = {1180--1187},
month = mar,
publisher = {American Medical Association},
doi = {10.1001/jama.2010.310},
read = {Yes},
rating = {0},
date-added = {2013-04-11T03:52:47GMT+00:00},
date-modified = {2013-04-20T16:08:59GMT+00:00},
abstract = {Abstract Context Theory and simulation suggest that randomized controlled trials (RCTs) stopped early for benefit (truncated RCTs) systematically overestimate treatment effects for the outcome that precipitated early stopping . Objective To compare the treatment effect  ... 
},
url = {http://jama.ama-assn.org/content/303/12/1180.short}
}

@article{Montori:2005bo,
author = {Montori, V M and Devereaux, P J and Adhikari, NKJ},
title = {{Randomized trials stopped early for benefit: a systematic review}},
journal = {JAMA},
year = {2005},
volume = {294},
number = {17},
pages = {2203--2209},
doi = {10.1001/jama.294.17.2203},
read = {Yes},
rating = {0},
date-added = {2013-04-11T03:53:33GMT+00:00},
date-modified = {2013-04-20T16:09:39GMT+00:00},
abstract = {Background Randomized clinical trials (RCTs) that stop earlier than planned because of apparent benefit often receive great attention and affect clinical practice. Their prevalence, the magnitude and plausibility of their treatment effects, and the extent to which they report ... 
},
url = {http://jama.jamanetwork.com/article.aspx?articleid=201802}
}

@article{Wainer:2007wr,
author = {Wainer, Howard},
title = {{The Most Dangerous Equation}},
journal = {American Scientist},
year = {2007},
volume = {95},
pages = {249--256},
month = apr,
read = {Yes},
rating = {0},
date-added = {2012-06-01T04:09:14GMT+00:00},
date-modified = {2013-04-20T16:10:30GMT+00:00},
abstract = {Ignorance of how sample size affects statistical variation has created havoc for nearly a millenium.}
}

@article{Gelman:1999gi,
author = {Gelman, A. and Price, P.N.},
title = {{All maps of parameter estimates are misleading}},
journal = {Statistics in Medicine},
year = {1999},
volume = {18},
number = {23},
pages = {3221--3234},
doi = {10.1002/(SICI)1097-0258(19991215)18:23<3221::AID-SIM312>3.0.CO;2-M},
read = {Yes},
rating = {0},
date-added = {2012-05-25T04:12:28GMT+00:00},
date-modified = {2013-04-20T16:11:04GMT+00:00},
abstract = {Maps are frequently used to display spatial distributions of parameters of interest, such as cancer rates or average pollutant concentrations by county. It is well known that plotting observed rates can have serious drawbacks when sample sizes vary by area, since very high (and low) observed rates are found disproportionately in poorly-sampled areas. Unfortunately, adjusting the observed rates to account for the effects of small-sample noise can introduce an opposite effect, in which the highest adjusted rates tend to be found disproportionately in well-sampled areas. In either case, the maps can be diffcult to interpret because the display of spatial variation in the underlying parameters of interest is confounded with spatial variation in sample sizes. As a result, spatial patterns occur in adjusted rates even if there is no spatial structure in the underlying parameters of interest, and adjusted rates tend to look too uniform in areas with little data. We introduce two models (normal and Poisson) in which parameters of interest have no spatial patterns, and demonstrate the existence of spatial artefacts in inference from these models. We also discuss spatial models and the extent to which they are subject to the same artefacts. We present examples from Bayesian modelling, but, as we explain, the artefacts occur generally.},
url = {http://www.stat.columbia.edu/~gelman/research/published/allmaps.pdf}
}

@article{Chan:2008bb,
author = {Chan, An-Wen and Hr{\'o}bjartsson, Asbj{\o}rn and J{\o}rgensen, Karsten J and G{\o}tzsche, Peter C and Altman, Douglas G},
title = {{Discrepancies in sample size calculations and data analyses reported in randomised trials: comparison of publications with protocols}},
journal = {BMJ},
year = {2008},
volume = {337},
pages = {a2299},
affiliation = {Mayo Clinic, Rochester, USA. chan.anwen@mayo.edu},
doi = {10.1136/bmj.a2299},
pmid = {19056791},
language = {English},
read = {Yes},
rating = {0},
date-added = {2012-05-30T04:44:06GMT+00:00},
date-modified = {2013-04-20T16:15:35GMT+00:00},
abstract = {OBJECTIVE:To evaluate how often sample size calculations and methods of statistical analysis are pre-specified or changed in randomised trials.

DESIGN:Retrospective cohort study. Data source Protocols and journal publications of published randomised parallel group trials initially approved in 1994-5 by the scientific-ethics committees for Copenhagen and Frederiksberg, Denmark (n=70).

MAIN OUTCOME MEASURE:Proportion of protocols and publications that did not provide key information about sample size calculations and statistical methods; proportion of trials with discrepancies between information presented in the protocol and the publication.

RESULTS:Only 11/62 trials described existing sample size calculations fully and consistently in both the protocol and the publication. The method of handling protocol deviations was described in 37 protocols and 43 publications. The method of handling missing data was described in 16 protocols and 49 publications. 39/49 protocols and 42/43 publications reported the statistical test used to analyse primary outcome measures. Unacknowledged discrepancies between protocols and publications were found for sample size calculations (18/34 trials), methods of handling protocol deviations (19/43) and missing data (39/49), primary outcome analyses (25/42), subgroup analyses (25/25), and adjusted analyses (23/28). Interim analyses were described in 13 protocols but mentioned in only five corresponding publications.

CONCLUSION:When reported in publications, sample size calculations and statistical methods were often explicitly discrepant with the protocol or not pre-specified. Such amendments were rarely acknowledged in the trial publication. The reliability of trial reports cannot be assessed without having access to the full protocols.},
url = {http://eutils.ncbi.nlm.nih.gov/entrez/eutils/elink.fcgi?dbfrom=pubmed&id=19056791&retmode=ref&cmd=prlinks}
}

@article{Gotzsche:2006du,
author = {G{\o}tzsche, Peter C},
title = {{Believability of relative risks and odds ratios in abstracts: cross sectional study}},
journal = {BMJ},
year = {2006},
volume = {333},
number = {7561},
pages = {231--234},
month = jul,
doi = {10.1136/bmj.38895.410451.79},
language = {English},
read = {Yes},
rating = {0},
date-added = {2012-05-30T04:32:37GMT+00:00},
date-modified = {2013-04-20T16:19:54GMT+00:00},
abstract = {Significant results in abstracts are common but should generally be disbelieved.},
url = {http://www.bmj.com/cgi/doi/10.1136/bmj.38895.410451.79}
}

@article{Bakker:2011ja,
author = {Bakker, Marjan and Wicherts, Jelte M},
title = {{The (mis)reporting of statistical results in psychology journals}},
journal = {Behavior Research Methods},
year = {2011},
volume = {43},
number = {3},
pages = {666--678},
month = apr,
publisher = {Springer-Verlag},
doi = {10.3758/s13428-011-0089-5},
language = {English},
read = {Yes},
rating = {0},
date-added = {2013-04-15T19:32:03GMT+00:00},
date-modified = {2013-04-20T16:20:35GMT+00:00},
abstract = {Abstract In order to study the prevalence, nature (direction), and causes of reporting errors in psychology , we checked the consistency of reported test statistics , degrees of freedom, and p values in a random sample of high-and low-impact psychology journals . In a second ... 
},
url = {http://www.springerlink.com/index/10.3758/s13428-011-0089-5}
}

@article{Baggerly:2009gk,
author = {Baggerly, Keith A and Coombes, Kevin R},
title = {{Deriving chemosensitivity from cell lines: Forensic bioinformatics and reproducible research in high-throughput biology}},
journal = {The Annals of Applied Statistics},
year = {2009},
volume = {3},
number = {4},
pages = {1309--1334},
month = dec,
doi = {10.1214/09-AOAS291},
read = {Yes},
rating = {0},
date-added = {2011-02-23T01:01:32GMT+00:00},
date-modified = {2013-04-20T16:24:54GMT+00:00},
abstract = {High-throughput biological assays such as microarrays let us ask very detailed questions about how diseases operate, and promise to let us personalize therapy. Data processing, however, is often not described well enough to allow for exact reproduction of the results, leading to exercises in ``forensic bioinformatics'' where aspects of raw data and reported results are used to infer what methods must have been employed. Unfortunately, poor documentation can shift from an inconvenience to an active danger when it obscures not just methods but errors. In this report we examine several related papers purporting to use microarray-based signatures of drug sensitivity derived from cell lines to predict patient response. Patients in clinical trials are currently being allocated to treatment arms on the basis of these results. However, we show in five case studies that the results incorporate several simple errors that may be putting patients at risk. One theme that emerges is that the most common errors are simple (e.g., row or column offsets); conversely, it is our experience that the most simple errors are common. We then discuss steps we are taking to avoid such errors in our own investigations.}
}

@article{Gotzsche:1989uy,
author = {G{\o}tzsche, Peter C},
title = {{Methodology and overt and hidden bias in reports of 196 double-blind trials of nonsteroidal antiinflammatory drugs in rheumatoid arthritis}},
journal = {Controlled Clinical Trials},
year = {1989},
volume = {10},
pages = {31--56},
month = jan,
read = {Yes},
rating = {0},
date-added = {2011-02-23T18:43:15GMT+00:00},
date-modified = {2013-04-20T16:26:16GMT+00:00},
abstract = {Important design aspects were decreasingly reported in NSAID trials over the years, whereas the quality of statistical analysis improved. In half of the trials, the effect variables in the methods and results sections were not the same, and the interpretation of the erythrocyte sedimentation rate in the reports seemed to depend on whether a significant difference was found. Statistically significant results appeared in 93 reports (47%). In 73 trials they favored only the new drug, and in 8 only the active control. All 39 trials with a significant difference in side effects favored the new drug. Choice of dose, multiple comparisons, wrong calculation, subgroup and within-groups analyses, wrong sampling units (in 63% of trials for effect variables, in 23% for side effects), change in measurement scale before analysis, baseline difference, and selective reporting of significant results were some of the verified or possible causes for the large proportion of results that favored the new drug.
Doubtful or invalid statements were found in 76% of the conclusions or abstracts. Bias consistently favored the new drug in 81 trials, and the control in only one trial.
It is not obvious how a reliable meta-analysis could be done in these trials.},
url = {http://linkinghub.elsevier.com/retrieve/pii/0197245689900172}
}

@article{Begley:2012,
author = {Begley, C. Glenn and Ellis, Lee M.},
title = {{Drug development: Raise standards for preclinical cancer research}},
journal = {Nature},
year = {2012},
volume = {483},
number = {7},
pages = {531--533},
affiliation = {C. Glenn Begley is a consultant and former vice-president and global head of Hematology and Oncology Research at Amgen, Thousand Oaks, California 91359, USA.},
doi = {10.1038/483531a},
read = {Yes},
rating = {0},
date-added = {2012-04-25T14:57:50GMT+00:00},
date-modified = {2012-07-04T19:09:57GMT+00:00},
abstract = {Efforts over the past decade to characterize the genetic alterations in human cancers have led to a better understanding of molecular drivers of this complex set of diseases. Although we in the cancer field hoped that this would lead to more effective drugs, historically, our ...}
}

@article{Hemenway:1997up,
author = {Hemenway, D},
title = {{Survey Research and Self-Defense Gun Use: An Explanation of Extreme Overestimates}},
journal = {The Journal of Criminal Law and Criminology},
year = {1997},
volume = {87},
number = {4},
pages = {1430--1445},
read = {Yes},
rating = {0},
date-added = {2012-12-31T19:03:44GMT+00:00},
date-modified = {2013-04-20T17:27:02GMT+00:00},
abstract = {Gary Kleck and Marc Gertz conducted a survey of civilian defensive gun use in 1992. In 1993, Kleck began publicizing the estimate that civilians use guns in self-defense against offenders up to 2.5 million times each year. This figure has been widely used by the National Rifle Association and by gun advocates. It is also often cited in the media2 and even in Congress. The Kleck and Gertz (K-G) paper has now been published. It is clear, however, that its conclusions cannot be accepted as valid.},
url = {http://www.jstor.org/stable/10.2307/1144020}
}

@article{Wicherts:2006jg,
author = {Wicherts, Jelte M and Borsboom, Denny and Kats, Judith and Molenaar, Dylan},
title = {{The poor availability of psychological research data for reanalysis}},
journal = {American Psychologist},
year = {2006},
volume = {61},
number = {7},
pages = {726--728},
doi = {10.1037/0003-066X.61.7.726},
language = {English},
read = {Yes},
rating = {0},
date-added = {2013-04-20T17:43:09GMT+00:00},
date-modified = {2013-04-20T18:41:33GMT+00:00},
url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/0003-066X.61.7.726}
}

@article{Wicherts:2011fp,
author = {Wicherts, Jelte M and Bakker, Marjan and Molenaar, Dylan},
title = {{Willingness to Share Research Data Is Related to the Strength of the Evidence and the Quality of Reporting of Statistical Results}},
journal = {PLoS ONE},
year = {2011},
volume = {6},
number = {11},
pages = {e26828},
month = nov,
publisher = {Public Library of Science},
doi = {10.1371/journal.pone.0026828},
read = {Yes},
rating = {0},
date-added = {2013-04-20T17:41:50GMT+00:00},
date-modified = {2013-04-20T18:59:51GMT+00:00},
url = {http://dx.plos.org/10.1371/journal.pone.0026828}
}

@article{Chan:2004gm,
author = {Chan, An-Wen and Hr{\'o}bjartsson, Asbj{\o}rn and Haahr, Mette T and G{\o}tzsche, Peter C and Altman, Douglas G},
title = {{Empirical Evidence for Selective Reporting of Outcomes in Randomized Trials: Comparison of Protocols to Published Articles}},
journal = {JAMA},
year = {2004},
volume = {291},
number = {20},
pages = {2457--2465},
month = may,
publisher = {American Medical Association},
doi = {10.1001/jama.291.20.2457},
read = {Yes},
rating = {0},
date-added = {2012-05-30T04:31:45GMT+00:00},
date-modified = {2013-04-20T20:02:47GMT+00:00},
abstract = {The reporting of trial outcomes is not only frequently incomplete but also biased and inconsistent with protocols. Published articles, as well as reviews that incorporate them, may therefore be unreliable and overestimate the benefits of an intervention. To ensure transparency, planned trials should be registered and protocols should be made publicly available prior to trial completion.},
url = {http://jama.jamanetwork.com/data/Journals/JAMA/4929/JOC32717.pdf}
}

@article{Plint:2006uj,
author = {Plint, A C and Moher, D and Morrison, A and Schulz, K and al, et},
title = {{Does the CONSORT checklist improve the quality of reports of randomised controlled trials? A systematic review}},
journal = {Medical journal of Australia},
year = {2006},
volume = {185},
number = {5},
pages = {263--267},
read = {Yes},
rating = {0},
date-added = {2013-04-15T18:22:45GMT+00:00},
date-modified = {2013-04-20T20:21:26GMT+00:00},
abstract = {Data synthesis: 1128 studies were retrieved, of which 248 were considered possibly 
relevant. Eight studies were included in the review. CONSORT adopters had significantly better 
reporting of the method of sequence generation (risk ratio [RR], 1.67; 95% CI, 1.19--2.33),  ... }
}

@article{Mills:2005ei,
author = {Mills, Edward and Wu, Ping and Gagnier, Joel and Heels-Ansdell, Diane and Montori, Victor M},
title = {{An analysis of general medical and specialist journals that endorse CONSORT found that reporting was not enforced consistently}},
journal = {Journal of Clinical Epidemiology},
year = {2005},
volume = {58},
number = {7},
pages = {662--667},
month = jul,
doi = {10.1016/j.jclinepi.2005.01.004},
pmid = {15939216},
language = {English},
rating = {0},
date-added = {2013-04-20T20:24:40GMT+00:00},
date-modified = {2013-04-20T20:24:50GMT+00:00},
abstract = {BACKGROUND: We aimed to determine if specialist journals implement specific Consolidated Standards for Reporting Trials ( CONSORT ) recommendations to the same extent as general medical journals . METHODS: Analysis of random controlled trials (RCTs ... 
},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0895435605000545}
}

@article{HuwilerMuntener:2002ij,
author = {Huwiler-M{\"u}ntener, Karin and J{\"u}ni, Peter and Junker, Christoph and Egger, Matthias},
title = {{Quality of Reporting of Randomized Trials as a Measure of Methodologic Quality}},
journal = {JAMA},
year = {2002},
volume = {287},
number = {21},
pages = {2801--2804},
month = jun,
publisher = {American Medical Association},
doi = {10.1001/jama.287.21.2801},
rating = {0},
date-added = {2013-04-20T20:36:23GMT+00:00},
date-modified = {2013-04-20T20:36:41GMT+00:00},
url = {http://jama.ama-assn.org/content/287/21/2801.short}
}

@article{Kyzas:2005ep,
author = {Kyzas, P A and Loizou, K T and Ioannidis, John P A},
title = {{Selective Reporting Biases in Cancer Prognostic Factor Studies}},
journal = {Journal of the National Cancer Institute},
year = {2005},
volume = {97},
number = {14},
pages = {1043--1055},
month = jul,
doi = {10.1093/jnci/dji184},
language = {English},
read = {Yes},
rating = {0},
date-added = {2012-11-30T19:27:18GMT+00:00},
date-modified = {2013-04-20T22:43:10GMT+00:00},
abstract = {Selective reporting may spuriously inflate the importance of postulated prognostic factors for various malignancies. We recommend that meta-analyses thereof should maximize retrieval of information and standardize definitions.},
url = {http://jnci.oxfordjournals.org/cgi/doi/10.1093/jnci/dji184}
}

@article{Eyding:2010bx,
author = {Eyding, Dirk and Lelgemann, Monika and Grouven, Ulrich and H{\"a}rter, Martin and Kromp, Mandy and Kaiser, Thomas and Kerekes, Michaela F and Gerken, Martin and Wieseler, Beate},
title = {{Reboxetine for acute treatment of major depression: systematic review and meta-analysis of published and unpublished placebo and selective serotonin reuptake inhibitor controlled trials}},
journal = {BMJ},
year = {2010},
volume = {341},
publisher = {BMJ Group},
doi = {10.1136/bmj.c4737},
pmid = {20940209},
language = {English},
rating = {0},
date-added = {2013-04-20T22:53:15GMT+00:00},
date-modified = {2013-04-20T22:57:37GMT+00:00},
abstract = {Objectives To assess the benefits and harms of reboxetine versus placebo or selective serotonin reuptake inhibitors (SSRIs) in the acute treatment of depression, and to measure the impact of potential publication bias in trials of reboxetine.}
}

@article{Prayle:2011cs,
author = {Prayle, Andrew P and Hurley, Matthew N and Smyth, Alan R},
title = {{Compliance with mandatory reporting of clinical trial results on ClinicalTrials.gov: cross sectional study}},
journal = {BMJ},
year = {2011},
volume = {344},
pages = {d7373},
doi = {10.1136/bmj.d7373},
language = {English},
rating = {0},
date-added = {2013-04-21T00:40:56GMT+00:00},
date-modified = {2013-04-21T00:47:24GMT+00:00},
abstract = {Objective To examine compliance with mandatory reporting of summary clinical trial results (within one year of completion of trial ) on ClinicalTrials. gov for studies that fall under the recent Food and Drug Administration Amendments Act (FDAAA) legislation.},
url = {http://www.bmj.com/content/344/bmj.d7373.pdf+html}
}

@article{Benjamini:1995ws,
author = {Benjamini, Yoav and Hochberg, Yosef},
title = {{Controlling the false discovery rate: a practical and powerful approach to multiple testing}},
journal = {Journal of the Royal Statistical Society Series B},
year = {1995},
pages = {289--300},
publisher = {JSTOR},
read = {Yes},
rating = {0},
date-added = {2013-04-09T01:00:52GMT+00:00},
date-modified = {2013-04-20T16:22:40GMT+00:00},
abstract = {The common approach to the multiplicity problem calls for controlling the familywise error rate (FWER). This approach, though, has faults, and we point out a few. A different approach to problems of multiple significance testing is presented. It calls for controlling the expected proportion of falsely rejected hypotheses -- the false discovery rate. This error rate is equivalent to the FWER when all hypotheses are true but is smaller otherwise. Therefore, in problems where the control of the false discovery rate rather than that of the FWER is desired, there is potential for a gain in power. A simple sequential Bonferroni-type procedure is proved to control the false discovery rate for independent test statistics, and a simulation study shows that the gain in power is substantial. The use of the new procedure and the appropriateness of the criterion are illustrated with examples.},
url = {http://www.jstor.org/stable/10.2307/2346101}
}

@article{Schroter:2008hw,
author = {Schroter, S and Black, N and Evans, S and Godlee, F and Osorio, L and Smith, R},
title = {{What errors do peer reviewers detect, and does training improve their ability to detect them?}},
journal = {JRSM},
year = {2008},
volume = {101},
number = {10},
pages = {507--514},
month = oct,
doi = {10.1258/jrsm.2008.080062},
language = {English},
read = {Yes},
rating = {0},
date-added = {2011-07-14T14:29:49GMT+00:00},
date-modified = {2013-04-23T02:06:23GMT+00:00},
url = {http://jrsm.rsmjournals.com/cgi/doi/10.1258/jrsm.2008.080062}
}

@article{Button:2013dz,
author = {Button, Katherine S and Ioannidis, John P A and Mokrysz, Claire and Nosek, Brian A and Flint, Jonathan and Robinson, Emma S J and Munaf{\`o}, Marcus R},
title = {{Power failure: why small sample size undermines the reliability of neuroscience}},
journal = {Nature Reviews Neuroscience},
year = {2013},
affiliation = {1] School of Experimental Psychology, University of Bristol, Bristol, BS8 1TU, UK. [2] School of Social and Community Medicine, University of Bristol, Bristol, BS8 2BN, UK.},
doi = {10.1038/nrn3475},
pmid = {23571845},
read = {Yes},
rating = {0},
date-added = {2013-04-15T15:42:54GMT+00:00},
date-modified = {2013-04-23T05:01:47GMT+00:00},
abstract = {A study with low statistical power has a reduced chance of detecting a true effect, but it is less well appreciated that low power also reduces the likelihood that a statistically significant result reflects a true effect. Here, we show that the average statistical power of studies in the neurosciences is very low. The consequences of this include overestimates of effect size and low reproducibility of results. There are also ethical dimensions to this problem, as unreliable research is inefficient and wasteful. Improving reproducibility in neuroscience is a key priority and requires attention to well-established but often ignored methodological principles.},
url = {http://eutils.ncbi.nlm.nih.gov/entrez/eutils/elink.fcgi?dbfrom=pubmed&id=23571845&retmode=ref&cmd=prlinks}
}

@article{Hauer:2004fz,
author = {Hauer, Ezra},
title = {{The harm done by tests of significance}},
journal = {Accident Analysis {\&} Prevention},
year = {2004},
volume = {36},
number = {3},
pages = {495--500},
month = may,
doi = {10.1016/S0001-4575(03)00036-8},
language = {English},
read = {Yes},
rating = {0},
date-added = {2013-03-26T19:35:46GMT+00:00},
date-modified = {2013-04-24T20:54:06GMT+00:00},
abstract = {Three historical episodes in which the application of null hypothesis significance testing (NHST) led to the mis-interpretation of data are described. It is argued that the pervasive use of this statistical ritual impedes the accumulation of knowledge and is unfit for use.},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0001457503000368}
}

@article{Preusser:1982gp,
author = {Preusser, David F and Leaf, William A and DeBartolo, Karen B and Blomberg, Richard D and Levy, Marvin M},
title = {{The effect of right-turn-on-red on pedestrian and bicyclist accidents}},
journal = {Journal of Safety Research},
year = {1982},
volume = {13},
number = {2},
pages = {45--55},
month = jun,
doi = {10.1016/0022-4375(82)90001-9},
language = {English},
read = {Yes},
rating = {0},
date-added = {2013-04-24T20:59:36GMT+00:00},
date-modified = {2013-04-24T21:02:30GMT+00:00},
abstract = {...  Turn -on- Red (RTOR), in its ``Western'' version allows motorists to turn  right on a red signal after stopping and yielding, unless specifically prohibited by a sign. The objective of this study was to determine the effect of Western RTOR on pedestrian and bicycle  accidents in selected ... 
},
url = {http://linkinghub.elsevier.com/retrieve/pii/0022437582900019}
}

@article{Schenker:2001cr,
author = {Schenker, Nathaniel and Gentleman, Jane F},
title = {{On judging the significance of differences by examining the overlap between confidence intervals}},
journal = {The American Statistician},
year = {2001},
volume = {55},
number = {3},
pages = {182--186},
publisher = {Taylor {\&} Francis},
doi = {10.1198/000313001317097960},
read = {Yes},
rating = {0},
date-added = {2013-04-25T01:23:38GMT+00:00},
date-modified = {2013-04-25T04:15:58GMT+00:00},
url = {http://www.jstor.org/stable/2685796}
}

@article{Belia:2005dg,
author = {Belia, S and Fidler, F and Williams, J and Cumming, G},
title = {{Researchers misunderstand confidence intervals and standard error bars}},
journal = {Psychological methods},
year = {2005},
volume = {10},
number = {4},
pages = {389--396},
doi = {10.1037/1082-989X.10.4.389},
read = {Yes},
rating = {0},
date-added = {2013-04-25T01:47:17GMT+00:00},
date-modified = {2013-04-25T04:20:13GMT+00:00},
abstract = {Little is known about researchers ' understanding of confidence intervals (CIs) and standard error (SE) bars . Authors of journal articles in psychology, behavioral neuroscience, and medicine were invited to visit a Web site where they adjusted a figure until they judged 2 ... 
},
url = {http://isites.harvard.edu/fs/docs/icb.topic477909.files/misunderstood_confidence.pdf}
}

@article{Lanzante:2005hi,
author = {Lanzante, John R},
title = {{A cautionary note on the use of error bars}},
journal = {Journal of climate},
year = {2005},
volume = {18},
number = {17},
pages = {3699--3703},
doi = {10.1175/JCLI3499.1},
read = {Yes},
rating = {0},
date-added = {2013-04-25T01:33:45GMT+00:00},
date-modified = {2013-04-25T04:23:42GMT+00:00},
url = {http://journals.ametsoc.org/doi/pdf/10.1175/JCLI3499.1}
}

@article{Gabriel:1978fp,
author = {Gabriel, K Ruben},
title = {{A simple method of multiple comparisons of means}},
journal = {Journal of the American Statistical Association},
year = {1978},
volume = {73},
number = {364},
pages = {724--729},
publisher = {Taylor {\&} Francis Group},
doi = {10.1080/01621459.1978.10480084},
read = {Yes},
rating = {0},
date-added = {2013-04-26T22:08:53GMT+00:00},
date-modified = {2013-04-26T22:15:17GMT+00:00},
url = {http://amstat.tandfonline.com/doi/full/10.1080/01621459.1978.10480084}
}

@article{Metz:2008hs,
author = {Metz, Anneke M},
title = {{Teaching Statistics in Biology: Using Inquiry-based Learning to Strengthen Understanding of Statistical Analysis in Biology Laboratory Courses}},
journal = {CBE Life Sciences Education},
year = {2008},
volume = {7},
pages = {317--326},
month = aug,
doi = {10.1187/cbe.07--07--0046},
read = {Yes},
rating = {0},
date-added = {2013-04-26T22:54:04GMT+00:00},
date-modified = {2013-04-26T22:55:14GMT+00:00},
url = {http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2527987/}
}

@article{Prinz:2011gb,
author = {Prinz, Florian and Schlange, Thomas and Asadullah, Khusru},
title = {{Believe it or not: how much can we rely on published data on potential drug targets?}},
journal = {Nature Reviews Drug Discovery},
year = {2011},
volume = {10},
pages = {328--329},
month = sep,
publisher = {Nature Publishing Group},
doi = {10.1038/nrd3439-c1},
read = {Yes},
rating = {0},
date-added = {2013-04-26T04:28:27GMT+00:00},
date-modified = {2013-04-27T00:19:07GMT+00:00},
abstract = {Nature Reviews Drug Discovery, (2011). doi:10.1038/nrd3439-c1},
url = {http://dx.doi.org/10.1038/nrd3439-c1}
}

@article{Schoenfeld:2013fq,
author = {Schoenfeld, J D and Ioannidis, John P A},
title = {{Is everything we eat associated with cancer? A systematic cookbook review}},
journal = {American Journal of Clinical Nutrition},
year = {2013},
volume = {97},
number = {1},
pages = {127--134},
month = jan,
doi = {10.3945/ajcn.112.047142},
language = {English},
read = {Yes},
rating = {0},
date-added = {2013-04-22T15:40:25GMT+00:00},
date-modified = {2013-04-27T05:04:35GMT+00:00},
url = {http://ajcn.nutrition.org/cgi/doi/10.3945/ajcn.112.047142}
}

@article{Tatsioni:2007cr,
author = {Tatsioni, Athina and Bonitsis, Nikolaos G and Ioannidis, John P A},
title = {{Persistence of Contradicted Claims in the Literature}},
journal = {JAMA},
year = {2007},
volume = {298},
number = {21},
pages = {2517--2526},
month = dec,
publisher = {American Medical Association},
doi = {10.1001/jama.298.21.2517},
read = {Yes},
rating = {0},
date-added = {2013-04-21T04:17:08GMT+00:00},
date-modified = {2013-04-27T05:11:15GMT+00:00},
abstract = {Abstract Context Some research findings based on observational epidemiology are contradicted by randomized trials, but may nevertheless still be supported in some scientific circles. Objectives To evaluate the change over time in the content of citations for 2 highly ... 
},
url = {http://jama.ama-assn.org/content/298/21/2517.short}
}

@article{Gonon:2012do,
author = {Gonon, Francois and Konsman, Jan-Pieter and Cohen, David and Boraud, Thomas},
title = {{Why Most Biomedical Findings Echoed by Newspapers Turn Out to be False: The Case of Attention Deficit Hyperactivity Disorder}},
journal = {PLoS ONE},
year = {2012},
volume = {7},
number = {9},
pages = {e44275},
month = sep,
doi = {10.1371/journal.pone.0044275.t002},
language = {English},
read = {Yes},
rating = {0},
date-added = {2012-09-24T04:12:38GMT+00:00},
date-modified = {2013-04-27T05:13:50GMT+00:00},
url = {http://dx.plos.org/10.1371/journal.pone.0044275.t002}
}

@article{Marshall:2000hg,
author = {Marshall, Max and Lockwood, Austin and Bradley, Caroline and Adams, Clive and Joy, Claire and Fenton, Mark},
title = {{Unpublished rating scales: a major source of bias in randomised controlled trials of treatments for schizophrenia}},
journal = {The British Journal of Psychiatry},
year = {2000},
volume = {176},
number = {3},
pages = {249--252},
month = mar,
doi = {10.1192/bjp.176.3.249},
read = {Yes},
rating = {0},
date-added = {2011-02-23T01:01:27GMT+00:00},
date-modified = {2013-04-27T05:16:43GMT+00:00},
abstract = {Unpublished scales are a source of bias in schizophrenia trials.}
}

@article{Kirkham:2010kj,
author = {Kirkham, J J and Dwan, K M and Altman, Douglas G and Gamble, C and Dodd, S and Smyth, R and Williamson, P R},
title = {{The impact of outcome reporting bias in randomised controlled trials on a cohort of systematic reviews}},
journal = {BMJ},
year = {2010},
volume = {340},
number = {feb15 1},
pages = {c365--c365},
month = feb,
doi = {10.1136/bmj.c365},
language = {English},
read = {Yes},
rating = {0},
date-added = {2013-04-21T00:05:12GMT+00:00},
date-modified = {2013-04-27T05:22:38GMT+00:00},
abstract = {Objective To examine the prevalence of outcome reporting bias ---the selection for publication of a subset of the original recorded outcome variables on the basis of the results --- and its impact on Cochrane reviews . Design A nine point classification system for missing ... 
},
url = {http://www.bmj.com/cgi/doi/10.1136/bmj.c365}
}

@article{LeLorier:1997ww,
author = {LeLorier, J and Gregoire, G and Benhaddad, A},
title = {{Discrepancies between meta-analyses and subsequent large randomized, controlled trials}},
journal = {New England Journal of Medicine},
year = {1997},
month = jan,
read = {Yes},
rating = {0},
date-added = {2011-02-23T02:21:10GMT+00:00},
date-modified = {2013-04-27T05:25:31GMT+00:00},
abstract = {We compared the results of large randomized, controlled trials (involving 1000 patients or more) that were published in four journals (the New England Journal of Medicine, the Lancet, the Annals of Internal Medicine, and the Journal of the American Medical Association) with the results of meta-analyses published earlier on the same topics. Regarding the principal and secondary outcomes, we judged whether the findings of the randomized trials agreed with those of the corresponding meta-analyses, and we determined whether the study results were positive (indicating that treatment improved the outcome) or negative (indicating that the outcome with treatment was the same or worse than without it) at the conventional level of statistical significance (P<0.05).},
url = {http://content.nejm.org/cgi/content/abstract/337/8/536}
}

@article{Wagenmakers:2011tp,
author = {Wagenmakers, E and Wetzels, R},
title = {{Why psychologists must change the way they analyze their data: The case of psi}},
journal = {Journal of Personality and Social Psychology},
year = {2011},
month = jan,
read = {Yes},
rating = {0},
date-added = {2011-02-23T02:03:49GMT+00:00},
date-modified = {2013-04-27T05:30:09GMT+00:00},
abstract = {Abstract Does psi exist? In a recent article, Dr. Bem conducted nine studies with over a thousand participants in an attempt to demonstrate that future events retroactively affect people's responses. Here we discuss several limitations of Bem's experiments on psi; in particular, ...},
url = {http://people.psych.cornell.edu/~jec7/pcd%20pubs/wagenmakersetal.pdf}
}

@article{Galak:2012fd,
author = {Galak, Jeff and LeBoeuf, Robyn A and Nelson, Leif D and Simmons, Joseph P},
title = {{Correcting the past: Failures to replicate psi}},
journal = {Journal of Personality and Social Psychology},
year = {2012},
volume = {103},
number = {6},
pages = {933--948},
doi = {10.1037/a0029709},
language = {English},
read = {Yes},
rating = {0},
date-added = {2013-04-27T05:34:18GMT+00:00},
date-modified = {2013-04-27T05:34:49GMT+00:00},
url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/a0029709}
}

@article{Pereira:2011eg,
author = {Pereira, Tiago V and Ioannidis, John P A},
title = {{Statistically significant meta-analyses of clinical trials have modest credibility and inflated effects}},
journal = {Journal of Clinical Epidemiology},
year = {2011},
volume = {64},
number = {10},
pages = {1060--1069},
month = oct,
doi = {10.1016/j.jclinepi.2010.12.012},
language = {English},
read = {Yes},
rating = {0},
date-added = {2013-04-15T19:09:58GMT+00:00},
date-modified = {2013-04-27T05:39:32GMT+00:00},
abstract = {OBJECTIVE: To assess whether nominally statistically significant effects in meta - analyses of clinical trials are true and whether their magnitude is inflated . STUDY DESIGN AND SETTING: Data from the Cochrane Database of Systematic Reviews 2005 (issue 4) and ... 
},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0895435611000096}
}

@article{Heffner:1996vq,
author = {Heffner, Robert A and Butler, Mark J and Reilly, Colleen Keelan},
title = {{Pseudoreplication revisited}},
journal = {Ecology},
year = {1996},
volume = {77},
number = {8},
pages = {2558--2562},
publisher = {JSTOR},
read = {Yes},
rating = {0},
date-added = {2013-04-30T20:52:34GMT+00:00},
date-modified = {2013-05-01T00:17:21GMT+00:00},
url = {http://www.jstor.org/stable/10.2307/2265754}
}

@article{Yang:2006wf,
author = {Yang, Z and Schank, J C},
title = {{Women do not synchronize their menstrual cycles}},
journal = {Human Nature},
year = {2006},
volume = {17},
number = {4},
pages = {4330447},
read = {Yes},
rating = {0},
date-added = {2013-05-01T01:39:02GMT+00:00},
date-modified = {2013-05-01T01:39:43GMT+00:00},
abstract = {... Systems. American Journal of Primatologv 41:65 85. 2000a Can  Pseudo  Entrainment Explain the Synchrony of Estrous  Cycles among Golden Ham- sters (Mesocricetus auratus)? Hormones and Behavior 38:94-101. 2000b ... 
},
url = {http://link.springer.com/article/10.1007/s12110-006-1005-z}
}

@article{Schank:2009fo,
author = {Schank, Jeffrey C and Koehnle, Thomas J},
title = {{Pseudoreplication is a pseudoproblem}},
journal = {Journal of Comparative Psychology},
year = {2009},
volume = {123},
number = {4},
pages = {421--433},
doi = {10.1037/a0013579},
language = {English},
read = {Yes},
rating = {0},
date-added = {2013-04-30T21:02:10GMT+00:00},
date-modified = {2013-05-01T01:40:54GMT+00:00},
url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/a0013579}
}

@article{Hurlbert:1984uv,
author = {Hurlbert, Stuart H},
title = {{Pseudoreplication and the design of ecological field experiments}},
journal = {Ecological Monographs},
year = {1984},
volume = {54},
number = {2},
pages = {187--211},
publisher = {Eco Soc America},
read = {Yes},
rating = {0},
date-added = {2013-04-30T20:47:09GMT+00:00},
date-modified = {2013-05-01T01:47:56GMT+00:00},
abstract = {Pseudoreplication is defined as the use of inferential statistics to test for treatment effects with data from experiments where either treatments are not replicated (though samples may be) or replicates are not statistically independent. In ANOVA terminology, it is the testing for treatment effects with an error term inappropriate to the hypothesis being considered. Scrutiny of 176 experimental studies published between 1960 and the present revealed that pseudoreplication occurred in 27% of them, or 48% of all such studies that applied inferential statistics. The incidence of pseudoreplication is especially high in studies of marine benthos and small mammals. The critical features of controlled experimentation are reviewed. Nondemonic intrusion is defined as the impingement of chance events on an experiment in progress. As a safeguard against both it and preexisting gradients, interspersion of treatments is argued to be an obligatory feature of good design. Especially in small experiments, adequate interspersion can sometimes be assured only by dispensing with strict random- ization procedures. Comprehension of this conflict between interspersion and randomization is aided by distinguishing pre-layout (or conventional) and layout-specifit alpha (probability of type I error). Suggestions are offered to statisticians and editors of ecological j oumals as to how ecologists' understanding of experimental design and statistics might be improved.},
url = {http://www.esajournals.org/doi/abs/10.2307/1942661}
}

@article{McClintock:1971bh,
author = {McClintock, Martha K},
title = {{Menstrual synchrony and suppression}},
journal = {Nature},
year = {1971},
volume = {229},
pages = {244--245},
publisher = {Nature Publishing Group},
doi = {10.1038/229244a0},
read = {Yes},
rating = {0},
date-added = {2013-05-01T01:25:03GMT+00:00},
date-modified = {2013-05-01T01:57:54GMT+00:00}
}

@article{Freedman:1983dj,
author = {Freedman, David A},
title = {{A note on screening regression equations}},
journal = {The American Statistician},
year = {1983},
volume = {37},
number = {2},
pages = {152--155},
publisher = {Taylor {\&} Francis Group},
doi = {10.1080/00031305.1983.10482729},
read = {Yes},
rating = {0},
date-added = {2013-04-22T19:33:24GMT+00:00},
date-modified = {2013-05-02T03:52:36GMT+00:00}
}

@article{Nosek:2012ek,
author = {Nosek, B A and Spies, J R and Motyl, M},
title = {{Scientific Utopia: II. Restructuring Incentives and Practices to Promote Truth Over Publishability}},
journal = {Perspectives on Psychological Science },
year = {2012},
volume = {7},
number = {6},
pages = {615--631},
month = nov,
doi = {10.1177/1745691612459058},
language = {English},
read = {Yes},
rating = {0},
date-added = {2013-04-15T19:27:31GMT+00:00},
date-modified = {2013-05-02T21:39:09GMT+00:00},
abstract = {... Two of the present authors, Matt Motyl and Brian A. Nosek , share interests in political ideology. ... The ultimate publication, Motyl and Nosek (2012), served as one of Motyl's signature publications as he finished graduate school and entered the job market. ... 
}
}
